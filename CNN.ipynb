{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8xZ1TDEBd/DH5V6GQICZn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stealthOG/Skill_Lab/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convulotional Neural Network\n",
        "\n",
        "#### Ekwe Collins Odinaka"
      ],
      "metadata": {
        "id": "IBtGB8CID8QM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pi0t3zadD4J3"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.convolutional import Conv2D # to add convolutional layers\n",
        "from keras.layers.convolutional import MaxPooling2D # to add pooling layers\n",
        "from keras.layers import Flatten # to flatten data for fully connected layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Convolutional Layer with One set of convolutional and pooling layers"
      ],
      "metadata": {
        "id": "PWjy7FrZEpKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import data\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# reshape to be [samples][pixels][width][height]\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS5gNtiZEYcI",
        "outputId": "961c70d5-6c84-49be-ce0b-13863f09970c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255 # normalize training data\n",
        "X_test = X_test / 255 # normalize test data"
      ],
      "metadata": {
        "id": "8BVn2bWoEwf8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, let's convert the target variable into binary categories\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "num_classes = y_test.shape[1] # number of categories"
      ],
      "metadata": {
        "id": "HcV29bSgE00m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define Function"
      ],
      "metadata": {
        "id": "8HNqQDi1FGPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's define a function that creates our model. Let's start with one set of convolutional and pooling layers."
      ],
      "metadata": {
        "id": "0nzZnYqLFCjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolutional_model():\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16, (5, 5), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # compile model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "mND8kO_iE-C1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code above defines a function convolutional_model() that creates a convolutional neural network model for image classification. The model is a Sequential model, which means it is a linear stack of layers.\n",
        "\n",
        "The model starts with a Conv2D layer. This layer performs a 2D convolution operation on the input data, where the input data has dimensions (28, 28, 1), which correspond to (image width, image height, number of channels). The convolution is done with 16 filters, each of size (5, 5). The strides parameter determines the step size to use when moving the convolutional filters across the input data. The activation parameter specifies the activation function to use, which is the ReLU activation function in this case.\n",
        "\n",
        "Next, the model has a MaxPooling2D layer, which applies max pooling to the output of the Conv2D layer. Max pooling is a downsampling operation that reduces the dimensionality of the data by taking the maximum value over a pooling window for each channel. In this case, the pooling window has size (2, 2) and the stride is (2, 2), which means that the maximum value of each 2x2 block in the input data is taken and the resulting downsampled data has a size of (14, 14, 16).\n",
        "\n",
        "After the max pooling layer, the model has a Flatten layer, which flattens the 3D output of the max pooling layer into a 1D array. This is necessary because the next layer, a Dense layer, expects 1D input. The Dense layer has 100 units and uses the ReLU activation function.\n",
        "\n",
        "Finally, the model has a second Dense layer with num_classes units, where num_classes is the number of classes in the classification task. This layer uses the softmax activation function, which is commonly used in classification tasks to convert the output of the model into a probability distribution over the classes.\n",
        "\n",
        "The model is compiled using the Adam optimization algorithm and the categorical cross-entropy loss function. The categorical cross-entropy loss is a common choice for classification tasks with more than two classes. The model is also evaluated using the accuracy metric.\n",
        "\n",
        "Once the model is created and compiled, it is returned by the function.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DxLZLe99HDWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model\n",
        "model = convolutional_model()\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, verbose=2)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))"
      ],
      "metadata": {
        "id": "5Szv0Y0RHN3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Convolutional Layer with two sets of convolutional and pooling layers"
      ],
      "metadata": {
        "id": "qH4aFTroJRRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolutional_model2():\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(8, (2, 2), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "e0sZYJaMHmKL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model\n",
        "model = convolutional_model2()\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, verbose=2)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcwY_RDXJYms",
        "outputId": "5af75fe0-c3b3-47a0-d07b-07c4b274dd60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "300/300 - 24s - loss: 0.4676 - accuracy: 0.8640 - val_loss: 0.1272 - val_accuracy: 0.9628 - 24s/epoch - 80ms/step\n",
            "Epoch 2/20\n",
            "300/300 - 22s - loss: 0.1139 - accuracy: 0.9665 - val_loss: 0.0824 - val_accuracy: 0.9747 - 22s/epoch - 74ms/step\n",
            "Epoch 3/20\n",
            "300/300 - 22s - loss: 0.0862 - accuracy: 0.9743 - val_loss: 0.0672 - val_accuracy: 0.9802 - 22s/epoch - 75ms/step\n",
            "Epoch 4/20\n",
            "300/300 - 22s - loss: 0.0687 - accuracy: 0.9790 - val_loss: 0.0600 - val_accuracy: 0.9814 - 22s/epoch - 72ms/step\n",
            "Epoch 5/20\n",
            "300/300 - 21s - loss: 0.0599 - accuracy: 0.9822 - val_loss: 0.0542 - val_accuracy: 0.9824 - 21s/epoch - 71ms/step\n",
            "Epoch 6/20\n",
            "300/300 - 21s - loss: 0.0523 - accuracy: 0.9844 - val_loss: 0.0527 - val_accuracy: 0.9825 - 21s/epoch - 71ms/step\n",
            "Epoch 7/20\n",
            "300/300 - 22s - loss: 0.0469 - accuracy: 0.9855 - val_loss: 0.0407 - val_accuracy: 0.9871 - 22s/epoch - 74ms/step\n",
            "Epoch 8/20\n",
            "300/300 - 22s - loss: 0.0416 - accuracy: 0.9873 - val_loss: 0.0482 - val_accuracy: 0.9854 - 22s/epoch - 74ms/step\n",
            "Epoch 9/20\n",
            "300/300 - 23s - loss: 0.0385 - accuracy: 0.9882 - val_loss: 0.0387 - val_accuracy: 0.9867 - 23s/epoch - 76ms/step\n",
            "Epoch 10/20\n",
            "300/300 - 23s - loss: 0.0370 - accuracy: 0.9884 - val_loss: 0.0367 - val_accuracy: 0.9891 - 23s/epoch - 76ms/step\n",
            "Epoch 11/20\n",
            "300/300 - 22s - loss: 0.0324 - accuracy: 0.9896 - val_loss: 0.0347 - val_accuracy: 0.9892 - 22s/epoch - 73ms/step\n",
            "Epoch 12/20\n",
            "300/300 - 22s - loss: 0.0290 - accuracy: 0.9907 - val_loss: 0.0334 - val_accuracy: 0.9892 - 22s/epoch - 72ms/step\n",
            "Epoch 13/20\n",
            "300/300 - 22s - loss: 0.0270 - accuracy: 0.9914 - val_loss: 0.0385 - val_accuracy: 0.9875 - 22s/epoch - 72ms/step\n",
            "Epoch 14/20\n",
            "300/300 - 21s - loss: 0.0245 - accuracy: 0.9926 - val_loss: 0.0335 - val_accuracy: 0.9886 - 21s/epoch - 71ms/step\n",
            "Epoch 15/20\n",
            "300/300 - 21s - loss: 0.0221 - accuracy: 0.9932 - val_loss: 0.0340 - val_accuracy: 0.9895 - 21s/epoch - 71ms/step\n",
            "Epoch 16/20\n",
            "300/300 - 23s - loss: 0.0210 - accuracy: 0.9936 - val_loss: 0.0333 - val_accuracy: 0.9884 - 23s/epoch - 76ms/step\n",
            "Epoch 17/20\n",
            "300/300 - 23s - loss: 0.0206 - accuracy: 0.9931 - val_loss: 0.0318 - val_accuracy: 0.9890 - 23s/epoch - 75ms/step\n",
            "Epoch 18/20\n",
            "300/300 - 23s - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.0335 - val_accuracy: 0.9885 - 23s/epoch - 76ms/step\n",
            "Epoch 19/20\n",
            "300/300 - 22s - loss: 0.0168 - accuracy: 0.9949 - val_loss: 0.0409 - val_accuracy: 0.9871 - 22s/epoch - 73ms/step\n",
            "Epoch 20/20\n",
            "300/300 - 21s - loss: 0.0169 - accuracy: 0.9944 - val_loss: 0.0367 - val_accuracy: 0.9877 - 21s/epoch - 71ms/step\n",
            "Accuracy: 0.9876999855041504 \n",
            " Error: 1.230001449584961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0_PSBHndJj2O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}